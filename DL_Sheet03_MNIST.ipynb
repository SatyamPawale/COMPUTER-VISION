{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SatyamPawale/COMPUTER-VISION/blob/main/DL_Sheet03_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6964269d",
      "metadata": {
        "id": "6964269d"
      },
      "source": [
        "# Deep Learning - Winter Term 2025/2026\n",
        "\n",
        "<hr style=\"border:2px solid gray\">\n",
        "\n",
        "### Exercise Sheet 03\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27a783fb",
      "metadata": {
        "id": "27a783fb"
      },
      "source": [
        "## 4.1 Implement different optimizer for SimpleCNN\n",
        "In this exercise we will make use of the CNN model we have seen in the lecture. We will handcraft different optimizers and see how they work on MNIST dataset. We continue with our approach and try to 'dive into' PyTorch and make our own implementations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8ea732c",
      "metadata": {
        "id": "d8ea732c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d70ee362",
      "metadata": {
        "id": "d70ee362"
      },
      "outputs": [],
      "source": [
        "# Typical check - if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d43c0726",
      "metadata": {
        "id": "d43c0726"
      },
      "outputs": [],
      "source": [
        "# This will automatically download the data and store it in the folder \"data\".\n",
        "train_data = torchvision.datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,\n",
        "    transform = torchvision.transforms.ToTensor(),\n",
        "    download = True,\n",
        ")\n",
        "test_data = torchvision.datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    transform = torchvision.transforms.ToTensor()\n",
        ")\n",
        "\n",
        "# Let's check what we have.\n",
        "print(train_data)\n",
        "print()\n",
        "print(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d02689a",
      "metadata": {
        "id": "8d02689a"
      },
      "outputs": [],
      "source": [
        "# Now let's visualize some examples.\n",
        "fig = plt.figure(figsize=(8, 7.5))\n",
        "fig.tight_layout()\n",
        "rows, cols = 4, 5\n",
        "\n",
        "# For reproducibility.\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for i in range(rows * cols):\n",
        "    # Grab random example from training set.\n",
        "    sample_idx = torch.randint(len(train_data), (1, )).item()\n",
        "    x, y = train_data[sample_idx]\n",
        "\n",
        "    # Plot image with with target class.\n",
        "    fig.add_subplot(rows, cols, i + 1)\n",
        "    plt.title(\"class {}\".format(y))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(x.squeeze(), cmap=\"gray\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2604a2ab",
      "metadata": {
        "id": "2604a2ab"
      },
      "outputs": [],
      "source": [
        "# Now we transfer the data into data loaders -> data loaders\n",
        "# are very helpful. They will organize data in batches for us.\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data,\n",
        "    batch_size=128,\n",
        "    shuffle=True, #SGD\n",
        "    pin_memory=True, # can accelerate GPU training.\n",
        "    num_workers=1\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data,\n",
        "    batch_size=100,\n",
        "    shuffle=True,\n",
        "    num_workers=1\n",
        ")\n",
        "\n",
        "print(train_loader)\n",
        "print(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74c14a4e",
      "metadata": {
        "id": "74c14a4e"
      },
      "outputs": [],
      "source": [
        "# Now we setup up our first CNN model.\n",
        "class SimpleCNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # First convolution layer (Conv -> ReLU -> MaxPooling)\n",
        "        # The sequential instruction automatically\n",
        "        # chains incorporated operations.\n",
        "        self.conv1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=5,\n",
        "                stride=1,\n",
        "                padding=2,\n",
        "            ),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        # Second convolution layer.\n",
        "        self.conv2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=32,\n",
        "                kernel_size=5,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        # Now we add a fully connected layer that maps\n",
        "        # all channels from the remaining resolution (7x7)\n",
        "        # onto 10 units (each representing one class).\n",
        "        self.out = torch.nn.Linear(32 * 7 * 7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First conv\n",
        "        out = self.conv1(x)\n",
        "        # Second conv\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        # Now we flatten the output of conv2 match the linear\n",
        "        # mapping of the fully connected layer 32 * 7 * 7 -> 10.\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.out(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4415e95",
      "metadata": {
        "id": "b4415e95"
      },
      "outputs": [],
      "source": [
        "# We can create an instance of our model and put it on the target device.\n",
        "model = SimpleCNN().to(device)\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(model)\n",
        "print(\"num_params:\", num_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad54578b",
      "metadata": {
        "id": "ad54578b"
      },
      "outputs": [],
      "source": [
        "# We define the loss function we need for training.\n",
        "# Softmax is already implemented in CrossEntropyLoss\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "print(criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f6ffe1b-894e-4024-af90-2d960c30868f",
      "metadata": {
        "id": "4f6ffe1b-894e-4024-af90-2d960c30868f"
      },
      "source": [
        "Previous cells were used already in the lecture. Now you will implement three optimizers and use them to train the SimpleCNN on MNIST dataset. You can test your implementation by training the same model with PyTorch optimizers. They should behave similarly (small differences may occur).\n",
        "\n",
        "Normally, we would set up the optimizer using PyTorch with the following line:\n",
        "\n",
        "```optimizer = torch.optim.Adam(model.parameters(), lr=0.005)```\n",
        "\n",
        "But here, we want you to learn how things work under the hood. Hence, you will implement some optimizers and test them on MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d2d4bb-d29e-4332-8164-fe5295836dfc",
      "metadata": {
        "id": "75d2d4bb-d29e-4332-8164-fe5295836dfc"
      },
      "outputs": [],
      "source": [
        "# Basic Stochastic Gradient Descent\n",
        "# TODO: apply momentum to the weight update\n",
        "class SGD():\n",
        "\n",
        "    def __init__(self, params, momentum, lr):\n",
        "        # Initialize the input parameters\n",
        "        self.params   = list(params) # model.parameters() is a generator\n",
        "        self.lr       = lr\n",
        "        # TODO: initialize momentum terms\n",
        "        self.momentum = None\n",
        "\n",
        "\n",
        "    def zero_grad(self):\n",
        "        # TODO: zero all the gradients of the parameters\n",
        "        #       you can use '.zero_()' function\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # If we don't use this decorator (or \"with torch.no_grad():\")\n",
        "    # the weight updates would become part of the computational graph\n",
        "    # (what we don't want here).\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        # TODO: implement weight update\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Generates a string representation for an instance.\n",
        "    def __repr__(self):\n",
        "        return '\\n'.join([str(__class__.__name__), 'mommentum:' + str(self.momentum), 'lr: ' + str(self.lr)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e737569-1155-42d8-8239-75887669d5a4",
      "metadata": {
        "id": "4e737569-1155-42d8-8239-75887669d5a4"
      },
      "outputs": [],
      "source": [
        "class RMSprop():\n",
        "\n",
        "    def __init__(self, params, lr, gamma, eps):\n",
        "        # TODO: initialize the input values here.\n",
        "        #       you need to wrap the params in a list\n",
        "\n",
        "        # TODO: Initialize the uncentered varience tensors\n",
        "        #       Append the tensors to a list\n",
        "        #       We don't need to keep track of their gradients\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def zero_grad(self):\n",
        "        # TODO: zero all the gradients of the parameters\n",
        "        #       you can use '.zero_()' function\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        # TODO: apply weight update here.\n",
        "        #       Use '.sub_()' function to subtract the\n",
        "        #       gradients from the weights of the prev. step.\n",
        "        #       You can use the algorithm from LectureNotes03 - page 20\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '\\n'.join([str(__class__.__name__) + ' (',\n",
        "                          '   lr: ' + str(self.lr),\n",
        "                          '   gamma: ' + str(self.gamma),\n",
        "                          '   eps: ' + str(self.eps), ')'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72ce6aca-b603-4112-92da-c5817c412305",
      "metadata": {
        "id": "72ce6aca-b603-4112-92da-c5817c412305"
      },
      "outputs": [],
      "source": [
        "class Adam():\n",
        "\n",
        "    def __init__(self, params, lr = 0.001, betas = (0.9, 0.999), eps=1e-8):\n",
        "        # TODO: initialize the input values here.\n",
        "        #       you need to wrap the params in a list\n",
        "\n",
        "        # This is a counter for the exponent in bias correction\n",
        "        # Please make use of it.\n",
        "        self.ctr = 0\n",
        "\n",
        "        # TODO: Initialize the uncentered varience and momentum tensors\n",
        "        #       Append the tensors to a list\n",
        "        #       We don't need to keep track of their gradients\n",
        "\n",
        "\n",
        "    def zero_grad(self):\n",
        "        # TODO: zero all the gradients of the parameters\n",
        "        #       you can use '.zero_()' function\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        # TODO: apply weight update here with bias correction.\n",
        "        #       Use '.sub_()' function to subtract the gradients\n",
        "        #       gradients from the weights of the prev. step.\n",
        "        #       You can use the algorithm from LectureNotes03 - pages 21&22\n",
        "        self.ctr += 1\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '\\n'.join(\n",
        "            [str(__class__.__name__) + ' (',\n",
        "            '   lr: ' + str(self.lr),\n",
        "            '   betas: ' + str(self.betas),\n",
        "            '   eps: ' + str(self.eps), ')'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc5195db",
      "metadata": {
        "id": "cc5195db"
      },
      "outputs": [],
      "source": [
        "model = SimpleCNN().to(device)\n",
        "\n",
        "### Note: In order to compare the optimizers, you need to\n",
        "### initialize the model again. To do that, basically\n",
        "### run this cell again. The model is initialized\n",
        "### in the first line of this cell.\n",
        "\n",
        "# Compare SGD with your and Torch's implementation\n",
        "optimizer = SGD(model.parameters(), momentum=0.9, lr=0.1)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.1)\n",
        "\n",
        "# Compare RMSprop with your and Torch's implementation\n",
        "# optimizer = RMSprop(model.parameters(), lr=0.001, gamma=0.999, eps=1e-08)\n",
        "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.999, eps=1e-08)\n",
        "\n",
        "# Compare Adam with your and Torch's implementation\n",
        "# optimizer = Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
        "\n",
        "print(optimizer)\n",
        "\n",
        "# Now let's do the training loop.\n",
        "num_epochs = 2\n",
        "\n",
        "# Set model to train mode -> relevant later with dropout etc.\n",
        "model.train()\n",
        "\n",
        "# Get the number of iterations per epoch.\n",
        "total_steps = len(train_loader)\n",
        "\n",
        "# Epoch loop.\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Iterate over all mini batches.\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "\n",
        "        # Put data to device.\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Do forward pass.\n",
        "        y_pred = model(x)\n",
        "\n",
        "        # Calc loss. Note the target is an integer {0, ..., 9}.\n",
        "        # The loss function will automatically process this loss\n",
        "        # adequately (i.e. as we have used a onehot vector).\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        # Now we perform backprop. The gradient is of the entire\n",
        "        # batch is automatically averaged by default.\n",
        "        loss.backward()\n",
        "\n",
        "        # Perform optimizer step.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Clear gradients (.grad tensors of all parameter tensors)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Do some logging.\n",
        "        if (i + 1) % 20 == 0:\n",
        "            print(\n",
        "                \"Epoch [{}/{}], iteration [{}/{}], loss: {:.4f}\".format(\n",
        "                   epoch + 1, num_epochs, i + 1, total_steps, loss.item()\n",
        "                )\n",
        "            )\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f106d851",
      "metadata": {
        "id": "f106d851"
      },
      "outputs": [],
      "source": [
        "# Now lets test our model for generalization on unseen examples.\n",
        "# Activate test mode.\n",
        "model.eval()\n",
        "\n",
        "# Used later ...\n",
        "temp_x = None\n",
        "temp_y = None\n",
        "temp_y_pred = None\n",
        "\n",
        "# We don't need gradients for testing (faster, saves resources)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for x, y in test_loader:\n",
        "\n",
        "        # Put data to device.\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Do forward pass.\n",
        "        y_pred = model(x)\n",
        "\n",
        "        # Transform into integer.\n",
        "        y_pred = torch.max(y_pred, 1)[1].data.squeeze()\n",
        "\n",
        "        correct += (y_pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "        # remember last batch for later.\n",
        "        temp_x = x\n",
        "        temp_y = y\n",
        "        temp_y_pred = y_pred\n",
        "\n",
        "    acc = float(correct) / float(total)\n",
        "\n",
        "    print(\"Final average test accuracy: {:.2f}%\".format(acc * 100.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e2230ad",
      "metadata": {
        "id": "6e2230ad"
      },
      "outputs": [],
      "source": [
        "# Let's check some examples from the last test batch.\n",
        "fig = plt.figure(figsize=(8, 7.5))\n",
        "fig.tight_layout()\n",
        "rows, cols = 4, 5\n",
        "\n",
        "for i in range(rows * cols):\n",
        "    # Grab examples from the test batch set.\n",
        "    x = temp_x[i]\n",
        "    y = temp_y[i]\n",
        "    y_pred = temp_y_pred[i]\n",
        "\n",
        "    # Plot image with with target class.\n",
        "    fig.add_subplot(rows, cols, i + 1)\n",
        "\n",
        "    color = \"red\"\n",
        "    if y_pred == y:\n",
        "        color = \"green\"\n",
        "\n",
        "    plt.title(\"pred {} / gt {}\".format(y_pred, y), color=color)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(x.cpu().squeeze().numpy(), cmap=\"gray\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a497334d",
      "metadata": {
        "id": "a497334d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}